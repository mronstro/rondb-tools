#!/usr/bin/env python3

import ipaddress, json, math, os, re, shutil, subprocess, sys, threading, time
from pathlib import Path
from functools import cache
from dataclasses import dataclass

# Constants
NUM_CLUSTER_CONN_MYSQLD=1
NUM_CLUSTER_CONN_RDRS=2
NUM_CLUSTER_CONN_BENCH=1

# Config schema
def config_schema():
    config_variables = [
        # Name                         type         TF       AWS      Static
        ("bench_count",                'pos_int',   True,    True,    False ),
        ("bench_cpus_per_node",        'pos_int',   False,   False,   True  ),
        ("bench_disk_size",            'pos_int',   True,    True,    False ),
        ("bench_instance_type",        'string',    True,    True,    False ),
        ("bench_private_ips",          'ip_list',   False,   False,   True  ),
        ("bench_public_ips",           'ip_list',   False,   False,   True  ),
        ("cpu_platform",               'cpu_pl',    True,    True,    True  ),
        ("deployment_type",            'string',    False,   True,    True  ),
        ("glibc_version",              'string',    False,   True,    True  ),
        ("grafana_instance_type",      'string',    True,    True,    False ),
        ("grafana_private_ips",        'ip_list',   False,   False,   True  ),
        ("grafana_public_ips",         'ip_list',   False,   False,   True  ),
        ("mysqld_count",               'pos_int',   True,    True,    False ),
        ("mysqld_disk_size",           'pos_int',   True,    True,    False ),
        ("mysqld_instance_type",       'string',    True,    True,    False ),
        ("mysqld_private_ips",         'ip_list',   False,   False,   True  ),
        ("mysqld_public_ips",          'ip_list',   False,   False,   True  ),
        ("ndb_mgmd_instance_type",     'string',    True,    True,    False ),
        ("ndb_mgmd_private_ips",       'ip_list',   False,   False,   True  ),
        ("ndb_mgmd_public_ips",        'ip_list',   False,   False,   True  ),
        ("ndbmtd_count",               'pos_int',   True,    True,    False ),
        ("ndbmtd_disk_size",           'pos_int',   True,    True,    False ),
        ("ndbmtd_instance_type",       'string',    True,    True,    False ),
        ("ndbmtd_private_ips",         'ip_list',   False,   False,   True  ),
        ("ndbmtd_public_ips",          'ip_list',   False,   False,   True  ),
        ("node_user",                  'string',    False,   False,   True  ),
        ("num_azs",                    'pos_int',   True,    True,    False ),
        ("prometheus_disk_size",       'pos_int',   True,    True,    False ),
        ("prometheus_instance_type",   'string',    True,    True,    False ),
        ("prometheus_private_ips",     'ip_list',   False,   False,   True  ),
        ("prometheus_public_ips",      'ip_list',   False,   False,   True  ),
        ("rdrs_count",                 'pos_int',   True,    True,    False ),
        ("rdrs_disk_size",             'pos_int',   True,    True,    False ),
        ("rdrs_instance_type",         'string',    True,    True,    False ),
        ("rdrs_major_version",         'one_two',   False,   True,    True  ),
        ("rdrs_private_ips",           'ip_list',   False,   False,   True  ),
        ("rdrs_public_ips",            'ip_list',   False,   False,   True  ),
        ("region",                     'string',    True,    True,    False ),
        ("rondb_replicas",             'pos_int',   False,   True,    True  ),
        ("rondb_version",              'string',    False,   True,    True  ),
        ("ssh_key_file",               'string',    False,   False,   True  ),
    ]
    def ip_valid(ip):
        try: ipaddress.ip_address(ip); return True
        except ValueError: return False
    validators = {
        'pos_int': (lambda x: None if isinstance(x, int) and x > 0 else
                    "must be a positive integer"),
        'string': (lambda x: None if isinstance(x, str) and len(x) > 0 else
                   "must be a nonempty string"),
        'ip_list': (lambda x: None
                    if isinstance(x, list)
                    and all(isinstance(i, str) and ip_valid(i) for i in x)
                    else "must be a list of valid IP addresses"),
        'cpu_pl': (lambda x: None if x in ['arm64_v8', 'x86_64']
                   else "must be either 'arm64_v8' or 'x86_64'"),
        'one_two': (lambda x: None if x in [1, 2]
                    else "must be either 1 or 2"),
    }
    @dataclass(slots=True, frozen=True)
    class ConfigVar:
        validate: callable
        TF: bool
        AWS: bool
        static: bool
    return {name: ConfigVar(validators[validator_name], *rest)
            for [name, validator_name, *rest] in config_variables}

trueColor = not ('TERM_PROGRAM' in os.environ and
                 'Apple_Terminal' in os.environ['TERM_PROGRAM'])
def node_color(r, g, b, i): return "38;5;16;48;" + (
    f"2;{r};{g};{b}" if trueColor else f"5;{i}")
node_types = [
    #Node type,    number of NDB NodeIds,               R    G    B   256-color
    ("ndb_mgmd",   1,                       node_color( 21, 149, 230,  32)),
    ("ndbmtd",     1,                       node_color(  3, 193, 148,  42)),
    ("mysqld",     NUM_CLUSTER_CONN_MYSQLD, node_color(255, 227,  71, 227)),
    ("rdrs",       NUM_CLUSTER_CONN_RDRS,   node_color(233, 125, 103, 167)),
    ("prometheus", 0,                       node_color(178, 141, 248, 141)),
    ("grafana",    0,                       node_color(239, 161, 248, 212)),
    ("bench",      NUM_CLUSTER_CONN_BENCH,  node_color(255, 178,  75, 215)),
]
stdout_is_tty=sys.stdout.isatty()
def color(s, c, b=""): return f"\033[{c}m{b+s+b}\033[0m" if stdout_is_tty else s
def red(s): return color(s, '1;31')
def green(s): return color(s, '1;32')
def lnk(uri):
    if not stdout_is_tty: return uri
    # Make URI bold underlined, and if supported, clickable.
    return f"\033]8;;{uri}\033\\\033[1;4m{uri}\033[0m\033]8;;\033\\"

# Nodes
@cache
def all_nodes():
    nodes = []
    nodeid = 0
    for node_type, num_nodeids, clr in node_types:
        public_ips = get_deploy_output(f'{node_type}_public_ips')
        private_ips = get_deploy_output(f'{node_type}_private_ips')
        assert len(public_ips) == len(private_ips), "IP count mismatch"
        for i in range(len(public_ips)):
            nodes.append(Node(node_type,
                              i,
                              public_ips[i],
                              private_ips[i],
                              [(nodeid:=nodeid+1) for _ in range(num_nodeids)],
                              clr))
    return nodes
@cache
def max_node_name_len(): return max(len(n.name()) for n in all_nodes())
def nodes_of_type(node_type):
    return [n for n in all_nodes() if n.type == node_type]
def node_count(node_type): return len(nodes_of_type(node_type))
class Node:
    def __init__(self, type, idx, public_ip, private_ip, ndb_nodeids, clr):
        self.type = type
        self.idx = idx
        self.public_ip = public_ip
        self.private_ip = private_ip
        self.ndb_nodeids = ndb_nodeids
        self.clr = clr
    def name(self):
        ret = self.type
        if node_count(self.type) > 1:
            ret += f"_{self.idx}"
        return ret
    def tui_name(self):
        name = self.name()
        suffix = ' ' * (max_node_name_len() - len(name))
        return color(name, self.clr, " ") + suffix
def first(node_type):
    for node in all_nodes():
        if node.type == node_type:
            return node
    raise ValueError(f"No node of type {node_type} found")
def nodes_from_expression(expr):
    # expr is None for all nodes, or a comma-separated list of node names or
    # node types.
    if expr == None: return all_nodes()
    ret = []
    for node_desc in expr.split(","):
        described_nodes = [node for node in all_nodes()
                           if node.name() == node_desc
                           or node.type == node_desc ]
        if len(described_nodes) == 0:
            die(f"'{node_desc}' doesn't match any node name or type."
                " Try ./cluster_ctl list")
        ret += described_nodes
    # Deduplicate and order
    ret = [node for node in all_nodes() if node in ret ]
    return ret

# Config
@cache
def config():
    # Import config from ./config.py, validate and return
    import config as config_module
    c = config_module.config
    if "deployment_type" not in c:
        die("Config variable 'deployment_type' not found in config.py.")
    dt = c['deployment_type']
    if dt not in ["aws", "static"]:
        die("Config variable 'deployment_type' must be either 'aws' or"
            " 'static'.")
    schema = config_schema()
    included = {'aws': lambda x: x.AWS, 'static': lambda x: x.static}[dt]
    included_vars = [ var for var in schema if included(schema[var]) ]
    for var in c:
        if var not in schema:
            die(f"Unexpected variable '{var}' in config.py.")
        if var not in included_vars:
            die(f"Config variable '{var}' is not supported in deployment type"
                f" '{dt}'.")
    for var in included_vars:
        if var not in c:
            die(f"Config variable '{var}' not found in config.py.")
        req = schema[var].validate(c[var])
        if req is not None:
            die(f"Config variable '{var}' {req}.")
    ndbmtd_count = (c['ndbmtd_count']
                    if dt == "aws" else len(c['ndbmtd_public_ips']))
    if ndbmtd_count % c['rondb_replicas'] != 0:
        die(f"Config variable 'ndbmtd_count' ({c['ndbmtd_count']}) must be"
            f" divisible by rondb_replicas ({c['rondb_replicas']}).")
    if dt == "static" and len(c['rdrs_public_ips']) > 1:
        # In order to support multiple rdrs nodes in static deployment, we need
        # to add load balancers and config variables rdrs_nlb_dns and
        # rondis_nlb_dns.
        die("Error: Static deployment does not support multiple RDRS nodes.")
    return c
def get_config(var_name):
    c = config()
    if var_name in c:
        return c[var_name]
    die(f"Error: config variable '{var_name}' not found.")

# Deploy outputs
has_read_tf_output = False
@cache
def tf_output():
    global has_read_tf_output
    tf_output_file = Path("tf_output")
    state_file = Path("terraform.tfstate")
    if not state_file.exists():
        die("Error: terraform.tfstate does not exist."+
            " Perhaps you forgot to run './cluster_ctl terraform'?")
    if not (tf_output_file.exists() and
            state_file.exists() and
            state_file.stat().st_mtime < tf_output_file.stat().st_mtime):
        proc = subprocess.run("terraform output -json > tf_output",
                              shell=True,
                              check=False)
        if proc.returncode != 0:
            die(f"Error running terraform output: {proc.stdout + proc.stderr}")
    has_read_tf_output = True
    return json.loads(tf_output_file.read_text(encoding='utf-8'))
def get_deploy_output(output_name):
    if output_name == "workspace":
        return f"/home/{get_deploy_output('node_user')}/workspace"
    if output_name == "run_dir":
        return f"{get_deploy_output('workspace')}/rondb-run"
    if output_name == "durable_dir":
        return f"/home/{get_deploy_output('node_user')}/durable"
    dt = get_config('deployment_type')
    if dt == "aws":
        if output_name == "node_user":
            return "ubuntu"
        if output_name == "ssh_key_file":
            return f"{get_aws_key_name()}.pem"
        return tf_output()[output_name]['value']
    assert dt == "static"
    if output_name in config():
        return get_config(output_name)
    die(f"Bug: Don't know how to get deploy output '{output_name}'.")

@cache
def get_rnd_cache():
    secret_file = Path("secrets.json")
    if not secret_file.exists():
        secret_file.write_text(
            json.dumps({
                "gui_secret": os.urandom(10).hex(),
                "unique_suffix": os.urandom(8).hex(),
            }),
            encoding='utf-8')
    return json.loads(secret_file.read_text(encoding='utf-8'))

def get_gui_secret(): return get_rnd_cache()["gui_secret"]
def get_unique_suffix(): return get_rnd_cache()["unique_suffix"]
def get_aws_key_name():
    if get_config('deployment_type') == "static":
        die("Bug")
    return f"rondb_bench_key_{get_unique_suffix()}"

# Util functions
def w(name, *content):
    Path(name).write_text("\n".join(content) + "\n",
                          encoding='utf-8')
def mkdir(p): Path(p).mkdir(parents=True, exist_ok=True)

# Generate ./config_files
def generate_config_files():
    # Clean directory
    shutil.rmtree("config_files", ignore_errors=True)
    mkdir("config_files")
    # Extract some values
    NODE_USER = get_deploy_output('node_user')
    WORKSPACE = get_deploy_output('workspace')
    RUN_DIR = get_deploy_output('run_dir')
    DURABLE_DIR = get_deploy_output('durable_dir')
    CONFIG_FILES=f"/home/{NODE_USER}/config_files"
    NDB_MGMD_PRI = first('ndb_mgmd').private_ip
    NO_OF_REPLICAS = get_config('rondb_replicas')
    RONDB_VERSION = get_config('rondb_version')
    GLIBC_VERSION = get_config('glibc_version')
    CPU_PLATFORM = get_config('cpu_platform')
    TARBALL_NAME = f"rondb-{RONDB_VERSION}-linux-glibc{GLIBC_VERSION}-{CPU_PLATFORM}.tar.gz"
    RDRS_IP= (get_deploy_output('rdrs_nlb_dns')
              if node_count('rdrs') > 1
              else first('rdrs').private_ip)
    RONDIS_IP= (get_deploy_output('rondis_nlb_dns')
                if node_count('rdrs') > 1
                else first('rdrs').private_ip)
    GUI_SECRET = get_gui_secret()
    # Generate shell_vars
    shell_vars = {
        "NODE_USER": NODE_USER,
        "WORKSPACE": WORKSPACE,
        "CONFIG_FILES": CONFIG_FILES,
        "RUN_DIR": RUN_DIR,
        "DURABLE_DIR": DURABLE_DIR,
        "NGINX_ERROR_LOG": f"{RUN_DIR}/nginx/nginx_error.log",
        "TARBALL_NAME": TARBALL_NAME,
        "NDB_MGMD_PRI": NDB_MGMD_PRI,
        "MYSQLD_PUB_1": first('mysqld').public_ip,
        "MYSQLD_PRI_1": first('mysqld').private_ip,
        "RDRS_URI": f"http://{RDRS_IP}:4406",
        "RONDIS_LB": f"{RONDIS_IP}:6379",
        "RDRS_PRI_1": first('rdrs').private_ip,
        "RDRS_MAJOR_VERSION": f"{get_config('rdrs_major_version')}",
        "BENCH_PRI_1": first('bench').private_ip,
        "GRAFANA_PRI_1": first('grafana').private_ip,
        "GUI_SECRET": GUI_SECRET,
    }
    w("config_files/shell_vars",
      *[f"export {var}={value}" for var, value in shell_vars.items()])
    # Generate config.ini
    configini = [
        "[NDB_MGMD DEFAULT]",
        f"DataDir={RUN_DIR}/ndb_mgmd/data",
        "",
        "[NDBD DEFAULT]",
        "AutomaticThreadConfig=true",
        "AutomaticMemoryConfig=true",
        "MaxDMLOperationsPerTransaction=100000",
        "MaxNoOfConcurrentOperations=100000",
        f"FileSystemPath={RUN_DIR}/ndbmtd/ndb_data",
        f"FileSystemPathDD={RUN_DIR}/ndbmtd/ndb_disk_columns",
        f"DataDir={RUN_DIR}/ndbmtd/data",
        "",
        f"NoOfReplicas={NO_OF_REPLICAS}",
        "PartitionsPerNode=4",
    ]
    node_type_configini_header = {
        "ndb_mgmd":   "[NDB_MGMD]",
        "ndbmtd":     "[NDBD]",
        "mysqld":     "[MYSQLD]",
        "rdrs":       "[API]",
        "bench":      "[API]",
    }
    for node in all_nodes():
        if not node.type in node_type_configini_header: continue
        configini += [""]
        for nodeid in node.ndb_nodeids:
            configini += [
                node_type_configini_header[node.type],
                f"# {node.name()}",
                f"HostName={node.private_ip}",
                f"NodeId={nodeid}",
            ]
            if get_config('deployment_type') == "aws":
                configini += [
                    f"LocationDomainId={(node.idx % get_config('num_azs')) + 1}"
                ]
    w("config_files/config.ini", *configini)
    # Generate my.cnf
    w("config_files/my.cnf",
        "[mysqld]",
        "ndbcluster",
        "user=root",
        f"basedir={WORKSPACE}/rondb",
        f"datadir={RUN_DIR}/mysqld/data",
        f"log_error={RUN_DIR}/mysqld/data/mysql-error.log",
        "log_error_verbosity=3",
        "",
        "[mysql_cluster]",
        f"ndb-connectstring={NDB_MGMD_PRI}:1186",
      )
    for node in all_nodes():
        w(f"config_files/nodeinfo_{node.name()}",
          f"NODEINFO_ROLE={node.type}",
          f"NODEINFO_IDX={node.idx}",
          f"NODEINFO_NODEIDS=\"{' '.join(map(str, node.ndb_nodeids))}\"",
          )
    # Generate rdrs_*.json
    for node in nodes_of_type("rdrs"):
        rdrs_conf = {
            "RonDB": {
                "Mgmds": [{"IP": NDB_MGMD_PRI}],
                "ConnectionPoolSize": len(node.ndb_nodeids),
                "NodeIDs": node.ndb_nodeids,
            },
            "REST": {
                "NumThreads": 64,
                "UseCompression": False,
                "UseSingleTransaction": False,
            },
            "Rondis": {
                "Enable": True,
                "NumThreads": 32,
                "NumDatabases": 2,
                "Databases": [
                    {
                        "Index": 0,
                        "FastHCOUNT": True,
                    },
                    {
                        "Index": 1,
                        "FastHCOUNT": False,
                    },
                ],
            },
            "Security": {
                "InsecureAllowAll": True,
                "APIKey": {
                    "UseHopsworksAPIKeys": False,
                },
            },
        }
        if get_config('rdrs_major_version') == 1:
            # rdrs 1 does not support more than one data connection. Also, it
            # uses different connections for RonDB and RonDBMetadataCluster even
            # if they are configured identically. Therefore, they must be
            # configured with different (or no) node IDs.
            assert len(node.ndb_nodeids) == 2
            [data_nodeid, metadata_nodeid] = node.ndb_nodeids
            rdrs_conf["RonDB"]["ConnectionPoolSize"] = 1
            rdrs_conf["RonDB"]["NodeIDs"] = [data_nodeid]
            rdrs_conf["RonDBMetadataCluster"] = {
                "Mgmds": [{"IP": NDB_MGMD_PRI, "Port": 1186}],
                "ConnectionPoolSize": 1,
                "NodeIDs": [metadata_nodeid],
            }
            # rdrs 1 has GRPC enabled by default, let's turn it off
            rdrs_conf["GRPC"] = { "Enable": False }
            # rdrs 1 REST does not default to port 4406, despite documentation
            # claiming so.
            rdrs_conf["REST"]["ServerPort"] = 4406
        w(f"config_files/rdrs_{node.idx}.json", json.dumps(rdrs_conf, indent=4))
    # Generate prometheus.yml
    prometheus_yml = [
        "global:",
        "  scrape_interval: 15s",
        "scrape_configs:",
        "  - job_name: 'rdrs'",
        "    static_configs:",
    ]
    for node in nodes_of_type("rdrs"):
        prometheus_yml += [
            f"      - targets: ['{node.private_ip}:4406']",
            "        labels:",
            f"          instance: '{node.name()}'",
        ]
    prometheus_yml += [
        "  - job_name: 'mysqld'",
        "    static_configs:",
    ]
    for node in nodes_of_type("mysqld"):
        prometheus_yml += [
            f"      - targets: ['{node.private_ip}:9104']",
            "        labels:",
            f"          instance: '{node.name()}'",
        ]
    prometheus_yml += [
        "  - job_name: 'linux'",
        "    static_configs:",
    ]
    for node in all_nodes():
        prometheus_yml += [
            f"      - targets: ['{node.private_ip}:9100']",
            "        labels:",
            f"          instance: '{node.name()}'",
            f"          instance_type: '{node.type}'",
        ]
    w("config_files/prometheus.yml", *prometheus_yml)
    # Generate grafana/
    mkdir("config_files/grafana/dashboards")
    mkdir("config_files/grafana/datasources")
    mkdir("config_files/grafana/plugins")
    mkdir("config_files/grafana/alerting")
    w("config_files/grafana/grafana.ini",
      "[paths]",
      f"data = {RUN_DIR}/grafana",
      f"logs = {RUN_DIR}/grafana/logs",
      f"provisioning = {CONFIG_FILES}/grafana",
      "[auth]",
      "disable_login_form = true",
      "[auth.anonymous]",
      "enabled = true",
      "org_role = Editor",
      "[server]",
      f"root_url = http://{first('bench').public_ip}:8080/grafana/",
      "serve_from_sub_path = true",
      "[live]",
      f"allowed_origins = http://{first('bench').public_ip}:8080",
      )
    w("config_files/grafana/datasources/prometheus.yaml",
      "apiVersion: 1",
      "",
      "datasources:",
      "  - name: Prometheus",
      "    type: prometheus",
      "    access: proxy",
      f"    url: http://{first('prometheus').private_ip}:9090",
      "    isDefault: true",
      "    uid: prometheus",
      # Dashboards copied from hopsworks-helm use a different data source uid.
      # By defining a second data source with the same endpoint, we can support
      # both without changing the dashboards.
      "  - name: Prometheus (alias)",
      "    type: prometheus",
      "    access: proxy",
      f"    url: http://{first('prometheus').private_ip}:9090",
      "    isDefault: false",
      "    uid: PBFA97CFB590B2093",
      )
    w("config_files/grafana/dashboards/dashboards.yaml",
      "apiVersion: 1",
      "",
      "providers:",
      "  - name: 'RonDB dashboards'",
      "    orgId: 1",
      "    folder: ''",
      "    folderUid: ''",
      "    type: file",
      "    options:",
      f"      path: {CONFIG_FILES}/grafana/dashboards",
      )
    shutil.copytree("dashboards", "config_files/grafana/dashboards",
                    dirs_exist_ok=True)
    for f in Path("config_files/grafana/dashboards").glob(
            {1: "rdrs2*.json", 2: "rdrs1*.json"}[
                get_config('rdrs_major_version')]):
        f.unlink()
    # nginx config to expose grafana and locust on the first bench
    # node. This implements a simple cookie-based authentication. An
    # unauthenticated user receives 403 for all requests except
    # /secret=SECRET/path, which will set the authentication cookie and redirect
    # to /path.
    w("config_files/nginx.conf",
      f'pid {RUN_DIR}/nginx/nginx.pid;',
      # For forwarding, worker_rlimit_nofile = worker_connections * 2 FDs
       'worker_rlimit_nofile 8192;',
       'events {',
       '  worker_connections 4096;',
       '}',
       'http {',
       '  map $http_upgrade $connection_upgrade {',
       '    ""      close;',
       '    default upgrade;',
       '  }',
       '  map $http_cookie $gui_secret {',
       '      "~X-AUTH=([^;]+)" $1;',
       '      default "";',
       '  }',
      f'  include {CONFIG_FILES}/nginx-dynamic.conf;',
      f'  client_body_temp_path {RUN_DIR}/nginx/client_temp;',
      f'  proxy_temp_path       {RUN_DIR}/nginx/proxy_temp;',
      f'  fastcgi_temp_path     {RUN_DIR}/nginx/fastcgi_temp;',
      f'  uwsgi_temp_path       {RUN_DIR}/nginx/uwsgi_temp;',
      f'  scgi_temp_path        {RUN_DIR}/nginx/scgi_temp;',
       '  server {',
      f'    access_log {RUN_DIR}/nginx/nginx_access.log;',
       '    listen 8080;',
       '    location ~ ^/secret=([0-9a-f]+)(/.*)$ {',
       '      add_header Set-Cookie "X-AUTH=$1; Path=/";',
       '      return 302 $2;',
       '    }',
       '    location /grafana {',
       '      if ($grafana_access = 0) {',
       '        return 403;',
       '      }',
       '      proxy_http_version 1.1;',
       '      proxy_set_header Upgrade $http_upgrade;',
       '      proxy_set_header Connection $connection_upgrade;',
      f'      proxy_pass http://{first("grafana").private_ip}:3000;',
       '    }',
       '    location /locust/ {',
       '      if ($locust_http_port = 0) {',
       '        return 403;',
       '      }',
       '      proxy_http_version 1.1;',
       '      proxy_set_header Upgrade $http_upgrade;',
       '      proxy_set_header Connection $connection_upgrade;',
       '      rewrite ^/locust/(.*)$ /$1 break;',
       '      proxy_pass http://127.0.0.1:$locust_http_port;',
       '    }',
       '    location / {',
       '      return 404;',
       '    }',
       '  }',
       '}',
      )
    w("config_files/nginx-dynamic.conf",
       'map $gui_secret $grafana_access {',
      f'    "{GUI_SECRET}" 1;',
       '    default 0;',
       '}',
       'map $gui_secret $locust_http_port {',
      f'    "{GUI_SECRET}" 8089;',
       '    default 0;',
       '}',
      )

# Parallell processing and TUI
failed = False
def die(s):
    if s: print(red(s))
    exit(1)
def check_failed():
    if failed: die("Failed")
def stream_process(cmd, prefix, check=False):
    global failed
    def do_pipe(out, pipe_prefix):
        suppressed=0
        for line in out:
            if failed:
                suppressed += 1
            else:
                print(f"{pipe_prefix}{line.rstrip()}")
        out.close()
        if suppressed > 0:
                print(f"{pipe_prefix} {red('SUPPRESSED')} {suppressed} lines due to error elsewhere")
    proc = subprocess.Popen(
        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    stdout_thread = threading.Thread(
        target=do_pipe,
        args=(proc.stdout,f"{prefix} stdout: "))
    stdout_thread.start()
    stderr_thread = threading.Thread(
        target=do_pipe,
        args=(proc.stderr, f"{prefix} stderr: "))
    stderr_thread.start()
    proc.wait()
    stdout_thread.join()
    stderr_thread.join()
    if proc.returncode == 0:
        print(f"{prefix} {green('DONE')} {' '.join(cmd)}")
    else:
        print(f"{prefix} {red('EXIT')} code {proc.returncode}, command: {' '.join(cmd)}")
        failed = True
    if check: check_failed()
def update_status(node):
    proc = subprocess.run(ssh(node, 'bash ~/scripts/status.sh'),
                          capture_output=True,
                          text=True,
                          check=False)
    node.status = proc.stdout.strip() if proc.returncode == 0 else None
def connectivity_check(node):
    global failed
    attempts = 12
    wait_time = 5
    for attempt in range(attempts):
        proc = subprocess.run(ssh(node, 'echo OK'),
                              capture_output=True, text=True)
        if proc.returncode == 0 and proc.stdout.strip() == 'OK':
            return
        if attempt < attempts - 1:
            print(f"{node.tui_name()} Connectivity check {attempt + 1} {red('failed')}, waiting {wait_time} seconds before trying again...")
            time.sleep(wait_time)
    print(f"{node.tui_name()} Connectivity check {red('failed')} {attempts} times, giving up.")
    failed = True
# Allow up to 10 concurrent rsync processes to upload config files to nodes
upload_lock = threading.Semaphore(10)
# Allow up to 2 nodes to download from repo.hops.works concurrently
repo_lock = threading.Semaphore(2)
LOCK_REPO = 1
UNLOCK_REPO = 2
LOCK_UPLOAD = 3
UNLOCK_UPLOAD = 4
UPDATE_STATUS = 5
CONNECTIVITY_CHECK = 6
def run_for_one_node(node, cmdfun):
    cmds = cmdfun(node)
    for cmd in cmds:
        if cmd == CONNECTIVITY_CHECK: connectivity_check(node)
        elif cmd == LOCK_UPLOAD: upload_lock.acquire()
        elif cmd == UNLOCK_UPLOAD: upload_lock.release()
        elif cmd == LOCK_REPO: repo_lock.acquire()
        elif cmd == UNLOCK_REPO: repo_lock.release()
        elif cmd == UPDATE_STATUS: update_status(node)
        elif not failed: stream_process(cmd, node.tui_name())
def run_for_all(for_nodes, cmdfun):
    threads = [ threading.Thread(target=run_for_one_node, args=(node, cmdfun))
                for node in for_nodes ]
    for thread in threads: thread.start()
    for t in threads: t.join()
    check_failed()
def ssh_opts():
    return [
        "-i", get_deploy_output('ssh_key_file'),
        "-o", "StrictHostKeyChecking=no",
        "-q"]
def ssh(node, cmd=""):
    NODE_USER = get_deploy_output('node_user')
    return ["ssh", *ssh_opts(), f"{NODE_USER}@{node.public_ip}",
            *([] if cmd=="" else [cmd])]
def rsync(file, node):
    NODE_USER = get_deploy_output('node_user')
    return ["rsync", "-az", "--delete", "-e", f"ssh {' '.join(ssh_opts())}",
            file, f"{NODE_USER}@{node.public_ip}:"]

# deploy command
def do_deploy(expr):
    if get_config('deployment_type') == "aws":
        do_terraform()
    for_nodes = nodes_from_expression(expr)
    do_install(for_nodes)
    do_start_cluster(for_nodes)

# terraform command
def do_terraform():
    if get_config('deployment_type') != "aws":
        die("Error: 'terraform' command is only supported for AWS deployment.")
    if has_read_tf_output:
        die("Bug: Called do_terraform() after tf_output()")
    # Generate ./terraform.tfvars
    schema = config_schema()
    var_repr = lambda x: f'"{x}"' if isinstance(x, str) else f'{x}'
    w("terraform.tfvars",
      *[f'{var}={var_repr(get_config(var))}'
        for var in schema
        if schema[var].TF],
      f'key_name="{get_aws_key_name()}"',
      f'unique_suffix="{get_unique_suffix()}"',
      )
    # Create AWS SSH key pair and save to file, unless it already exists.
    key_name=get_aws_key_name()
    key_file=Path(get_deploy_output('ssh_key_file'))
    if not key_file.exists():
        print(f"Creating key pair {key_name}...")
        subprocess.run(f"aws ec2 create-key-pair --key-name {key_name}"
                       f" --key-type ed25519 --region {get_config('region')}"
                       f" --query KeyMaterial --output text > {key_file}",
                       shell=True, check=True)
        key_file.chmod(0o600)
    if not key_file.exists() or key_file.stat().st_size == 0:
        die("Failed to create key pair.")
    # Initialize terraform if necessary
    if not Path(".terraform").exists():
        print("Running terraform init...")
        if subprocess.run(["terraform", "init"], check=False).returncode != 0:
            die("'terraform init' failed.")
        print("Terraform initialized successfully.")
    # terraform apply
    if subprocess.run(["terraform", "apply", "-auto-approve"],
                      check=False
                      ).returncode != 0:
        die("'terraform apply' failed.")

# install command
def do_install(for_nodes):
    generate_config_files()
    print("Deployment preview:")
    print("--------------------------------------------------")
    for node in for_nodes:
        print(f"{node.tui_name()} -> {node.private_ip} / {node.public_ip}")
    print("--------------------------------------------------")
    print("Installing...")
    run_for_all(for_nodes, lambda node: [
        CONNECTIVITY_CHECK,
        ssh(node,
            "if [ -f /tmp/rsync-installed ]; then "
            "  :; "
            "else "
            "  sudo DEBIAN_FRONTEND=noninteractive apt-get update -yq && "
            "  sudo DEBIAN_FRONTEND=noninteractive "
            "    apt-get install -yq rsync libncurses6; "
            "fi && "
            "touch /tmp/rsync-installed"),
        LOCK_UPLOAD,
        rsync("./scripts", node),
        rsync("./config_files", node),
        UNLOCK_UPLOAD,
        ssh(node, f"cp config_files/nodeinfo_{node.name()} config_files/nodeinfo"),
        LOCK_REPO,
        ssh(node, "bash ~/scripts/download_tarball.sh"),
        UNLOCK_REPO,
        ssh(node, "bash ~/scripts/install.sh"),
    ])

# start command
def do_start_cluster(for_nodes):
    print("Before starting services, make sure they are stopped.")
    do_stop_cluster(for_nodes)
    print_uri = False
    for start_type, _, _ in node_types:
        nodes_this_iteration = [n for n in for_nodes if n.type == start_type]
        count = len(nodes_this_iteration)
        if count == 0: continue
        if start_type == "bench": print_uri = True
        print("Starting services on the" +
              (f" {count} " if count>1 else " ") +
              color(start_type, first(start_type).clr, " ") +
              f" node{'s' if count>1 else ''}...")
        run_for_all(nodes_this_iteration, lambda node: [
            ssh(node, f"bash ~/scripts/start_{node.type}.sh")
        ])
    if print_uri:
        uri=lnk(f"http://{first('bench').public_ip}:8080"
                f"/secret={get_gui_secret()}/grafana/d"
                f"/rdrs{get_config('rdrs_major_version')}_overview")
        print(f'Go to {uri} in your browser to see the dashboard.')

# stop command
def do_stop_cluster(for_nodes):
    run_for_all(for_nodes, lambda node: [
        ssh(node, "bash ~/scripts/cleanup.sh")
    ])

# open_tmux command
def do_open_tmux(for_nodes):
    WORKSPACE = get_deploy_output('workspace')
    RUN_DIR = get_deploy_output('run_dir')
    if not shutil.which("tmux"):
        die("tmux is not installed.")
    def tmux(*args, check=True, **kwargs):
        cmd = ["tmux", *args]
        return subprocess.run(cmd, check=check, **kwargs).returncode == 0
    session_name = f"rondb_bm {get_unique_suffix()}"
    if not tmux("has-session", "-t", session_name, capture_output=True, check=False):
        tmux("new-session", "-d", "-s", session_name)
        for node in for_nodes:
            name = node.name()
            tmux("new-window", "-t", session_name, "-n", name, *ssh(node))
            line=f"cd {RUN_DIR}/{node.type}"
            if node.type == "bench":
                line=f"cd {RUN_DIR}"
            if node.type == "mysqld":
                line=f"cd {RUN_DIR}/{node.type}; alias mysql=\"{WORKSPACE}/rondb/bin/mysql -uroot\""
            tmux("send-keys", "-t", f"{session_name}:{name}", line, "C-m")
        tmux("kill-window", "-t", f"{session_name}:0")
    tmux("attach-session", "-t", session_name)

# ssh command
def do_ssh(nodename, *cmd):
    for node in all_nodes():
        if not node.name() == nodename: continue
        exit(subprocess.run(ssh(node, ' '.join(cmd)),
                            check=False
                            ).returncode)
    die("Could not find a node by that name. Try ./cluster_ctl list")

# kill_tmux command
def do_kill_tmux():
    if not shutil.which("tmux"):
        die("tmux is not installed.")
    if not Path("secrets.json").exists():
        # Avoid creating secrets.json unnecessarily.
        print("No tmux session to delete.")
        return
    session_name = f"rondb_bm {get_unique_suffix()}"
    if subprocess.run(["tmux", "has-session", "-t", session_name],
                      capture_output=True,
                      check=False
                      ).returncode != 0:
        print("No tmux session to delete.")
        return
    print(f"Deleting tmux session '{session_name}'.")
    subprocess.run(["tmux", "kill-session", "-t", session_name], check=True)
    print("Done deleting tmux session.")

# bench_locust command
def do_bench_locust(columns, rows, column_types, total_workers):
    do_start_locust(rows, total_workers, only_validate=True)
    do_populate(columns, rows, column_types)
    do_start_locust(rows, total_workers)

# populate command
def do_populate(columns, rows, column_types):
    # Validate parameters
    first('mysqld') # Assert that one exists
    if columns < 1 or 200 < columns:
        die("NUM_COLUMNS must be in the range 1 - 200")
    if rows < 1 or 1000000000 < rows:
        die("NUM_ROWS must be in the range 1 - 1,000,000,000")
    if column_types not in ['INT', 'VARCHAR', 'INT_AND_VARCHAR', 'VARCHAR_250', 'VARBINARY_2000', 'VARBINARY_29500', 'VARCHAR_2000','VARCHAR_29500']:
        die("COLUMN_TYPES must be one of: INT, VARCHAR, INT_AND_VARCHAR, VARCHAR_250, VARBINARY_2000, VARBINARY_29500,VARCHAR_2000,VARCHAR_29500")
    batch_size = max(math.floor(10000 / columns), 1)
    column_info = {'INT': 1, 'VARCHAR': 2, 'VARCHAR_250': 3, 'VARBINARY_2000': 4, 'VARBINARY_29500': 5, 'VARCHAR_2000': 6, 'VARCHAR_29500': 7, 'INT_AND_VARCHAR': 0}[column_types]
    # Populate table
    run_for_one_node(first('mysqld'), lambda node: [
        ssh(node, f"bash ~/scripts/populate_locust.sh {columns} {rows}" +
            f" {batch_size} {column_info}")
    ])
    check_failed()

# start_locust command
def do_start_locust(rows, total_workers, only_validate=False):
    # Validate parameters
    if rows < 1 or 1000000000 < rows:
        die("NUM_ROWS must be in the range 1 - 1,000,000,000")
    num_bench_nodes = node_count("bench")
    cpus_per_node = get_deploy_output('bench_cpus_per_node')
    cpus = num_bench_nodes * cpus_per_node
    max_workers = cpus - 1 # Reserve one CPU for master
    min_workers = max(1, num_bench_nodes - 1)
    if max_workers < min_workers:
        die("Error: Not enough CPUs for workers.")
    if total_workers == None: total_workers = max_workers
    if total_workers < min_workers or max_workers < total_workers:
        die(f"Error: Total workers must be in the range {min_workers} - {max_workers}")
    if only_validate: return
    # Stop
    do_stop_locust()
    # Start
    def cmdfun(node):
        num_workers = math.floor(total_workers / num_bench_nodes)
        if ((total_workers - num_workers * num_bench_nodes) >=
            (num_bench_nodes - node.idx)):
            num_workers += 1
        return [ ssh(node, f"bash ~/scripts/start_locust.sh {rows}" +
                     f" {num_workers}") ]
    run_for_all(nodes_of_type("bench"), cmdfun)
    # Print URI
    uri=lnk(f"http://{first('bench').public_ip}:8080/secret={get_gui_secret()}/locust")
    print(f"Go to {uri} in your browser to start the benchmark")

# stop_locust command
def do_stop_locust():
    run_for_all(nodes_of_type("bench"), lambda node: [
        ssh(node, "source ~/scripts/include.sh && stop locust")
    ])

# list command
def do_list():
    run_for_all(all_nodes(), lambda _: [UPDATE_STATUS])
    rows = [[
        color("Name", "1;4", " "),
        color('Type', "1;4"),
        color('Public IP', "1;4"),
        color('Private IP', "1;4"),
        color('NDB Node IDs', "1;4"),
        color('Status', "1;4"),
    ]]
    for node in all_nodes():
        rows.append([
            node.tui_name(),
            node.type,
            node.public_ip,
            node.private_ip,
            ', '.join(map(str, node.ndb_nodeids)),
            node.status if node.status is not None else red('ERROR'),
        ])
    def width(s): return len(re.sub(r'\033\[[0-9;]*m', '', s))
    maxwidths = [ max(width(row[i]) for row in rows) for i in range(len(rows[0])) ]
    for row in rows:
        print('   '.join(s + ' ' * (maxwidths[i] - width(s))
                       for i, s in enumerate(row)).rstrip())

# cleanup command
def cleanup_aws():
    # Destroy terraform cluster if it exists
    state_file = Path("terraform.tfstate")
    if state_file.exists():
        print("Destroying terraform cluster.")
        if subprocess.run(["terraform", "destroy", "-auto-approve"],
                          check=False,
                          ).returncode != 0:
            die("'terraform destroy' failed.")
        print("Done destroying terraform cluster.")
    else:
        print("Skip 'terraform destroy' since there is no terraform.tfstate"
              " file.")
    # Destroy AWS SSH key pair if it exists
    key_name=get_aws_key_name()
    key_file=Path(get_deploy_output('ssh_key_file'))
    if key_file.exists():
        print("Deleting AWS SSH key.")
        subprocess.run(["aws", "ec2", "delete-key-pair",
                        "--key-name", key_name,
                        "--region", get_config('region')],
                       check=True)
        key_file.unlink()
        print("Done deleting AWS SSH key.")
    else:
        print("No AWS SSH key to delete.")
    cleanup_common()
def cleanup_static():
    cleanup_common()
    print("Cannot destroy nodes, since they are provided statically. You'll have to do that manually.")
def cleanup_common():
    if shutil.which("tmux"):
        do_kill_tmux()
    print("Deleting temporary files.")
    subprocess.run("rm -rf .terraform* terraform.* tf_output config_files"
                   " __pycache__ secrets.json",
                   shell=True, check=True)
    print("Done deleting temporary files.")
def do_cleanup():
    {
        "aws": cleanup_aws,
        "static": cleanup_static,
    }[get_config('deployment_type')]()

# CLI
def main():
    argc = len(sys.argv)
    node_type_names = [x[0] for x in node_types]
    if argc < 2:
        print('\n'.join([
            "Usage:",
            "  ./cluster_ctl deploy [NODES]",
            "    Configure, initialize and apply terraform, if needed. Then install and start",
            "    RonDB services.",
            "      NODES: See bottom.",
            "",
            "  ./cluster_ctl terraform",
            "    Configure, initialize and apply terraform, as needed.",
            "",
            "  ./cluster_ctl install [NODES]",
            "    Install RonDB services without starting. Called by deploy.",
            "      NODES: See bottom.",
            "",
            "  ./cluster_ctl start [NODES]",
            "    Start or restart RonDB services, without installing. Called by deploy.",
            "      NODES: See bottom.",
            "",
            "  ./cluster_ctl stop [NODES]",
            "    Stop RonDB services. Called by start (and hence deploy).",
            "      NODES: See bottom.",
            "",
            "  ./cluster_ctl open_tmux [NODES]",
            "    Attach to a tmux session with ssh connections opened to all applicable",
            "    nodes. If a session does not exist, it is created according to the NODES",
            "    parameter, otherwise the parameter is ignored. To connect to a different set",
            "    of nodes, use kill_tmux first.",
            "      NODES: See bottom.",
            "",
            "  ./cluster_ctl kill_tmux",
            "    Kill the tmux session created by open_tmux.",
            "",
            "  ./cluster_ctl ssh NODE_NAME",
            "    Open an ssh connection to a node.",
            "",
            "  ./cluster_ctl bench_locust [--cols COLUMNS] [--rows ROWS]\\",
            "                             [--types COLUMN_TYPES] [--workers WORKERS]",
            "    Populate a table with test data and start locust ready to run a benchmark",
            "    against it.",
            "      COLUMNS: Number of columns in table. Default 10.",
            "      ROWS: Number of rows in table. Default 100000.",
            "      COLUMN_TYPES: The columns types in table; one of INT, VARCHAR,",
            "                    INT_AND_VARCHAR. Default INT.",
            "      WORKERS: Number of locust workers. Defaults to the maximum possible.",
            "",
            "  ./cluster_ctl populate [--cols COLUMNS] [--rows ROWS] [--types COLUMN_TYPES]",
            "    Populate a table with test data. Called by bench_locust.",
            "      COLUMNS: Number of columns in table. Default 10.",
            "      ROWS: Number of rows in table. Default 100000.",
            "      COLUMN_TYPES: The columns types in table; one of INT, VARCHAR,",
            "                    INT_AND_VARCHAR. Default INT.",
            "",
            "  ./cluster_ctl start_locust [--rows ROWS] [--workers WORKERS]\\",
            "    Start locust services. Called by bench_locust.",
            "      ROWS: Number of rows in table. Default 100000. MUST be set to the same",
            "            value as the last call to populate. Prefer using bench_locust, which",
            "            will do the right thing automatically.",
            "      WORKERS: Number of locust workers. Defaults to the maximum possible.",
            "",
            "  ./cluster_ctl stop_locust",
            "    Stop locust. Called by start_locust (and hence bench_locust).",
            "",
            "  ./cluster_ctl list",
            "    List node information.",
            "",
            "  ./cluster_ctl cleanup",
            "    Destroy AWS resources (if any) and delete temporary files and tmux session.",
            "",
            "NODES: A comma-separated list of node names and node types. If given, operate",
            "       only on matching nodes, otherwise on all nodes. Available node types are:",
            "       "+', '.join(node_type_names),
        ]))
        die("")
    action = sys.argv[1]
    if action in ["deploy", "install", "start", "stop", "open_tmux"]:
        arg = None
        if argc == 2: pass
        elif argc == 3:
            arg = sys.argv[2]
        else: die(f"Usage: ./cluster_ctl {action} [NODES]")
        if action != "deploy": arg = nodes_from_expression(arg)
        if action == "deploy": do_deploy(arg)
        elif action == "install": do_install(arg)
        elif action == "start": do_start_cluster(arg)
        elif action == "stop": do_stop_cluster(arg)
        elif action == "open_tmux": do_open_tmux(arg)
        else: die("Bug")
    elif action == "ssh":
        if argc < 3:
            die("Usage: ./cluster_ctl ssh NODE_NAME [COMMAND ...]")
        do_ssh(*sys.argv[2:])
    elif action in ["terraform",
                    "kill_tmux",
                    "stop_locust",
                    "list",
                    "cleanup"]:
        if argc != 2:
            die(f"Usage: ./cluster_ctl {action}")
        if action == "terraform": do_terraform()
        elif action == "kill_tmux": do_kill_tmux()
        elif action == "stop_locust": do_stop_locust()
        elif action == "list": do_list()
        elif action == "cleanup": do_cleanup()
        else: die("Bug")
    elif action in ["bench_locust", "populate", "start_locust"]:
        columns = 10
        rows = 100000
        column_types = "INT"
        total_workers = None
        i = 2
        while i < argc:
            arg = sys.argv[i]
            if arg == "--cols" and action != "start_locust":
                i += 1
                if i >= argc: die("Missing value after --cols")
                columns = int(sys.argv[i])
            elif arg == "--rows":
                i += 1
                if i >= argc: die("Missing value after --rows")
                rows = int(sys.argv[i])
            elif arg == "--types" and action != "start_locust":
                i += 1
                if i >= argc: die("Missing value after --types")
                column_types = sys.argv[i]
            elif arg == "--workers" and action != "populate":
                i += 1
                if i >= argc: die("Missing value after --workers")
                total_workers = int(sys.argv[i])
            else:
                die(f"Unknown argument: {arg}")
            i += 1
        if action == "bench_locust":
            do_bench_locust(columns, rows, column_types, total_workers)
        elif action == "populate":
            do_populate(columns, rows, column_types)
        elif action == "start_locust":
            do_start_locust(rows, total_workers)
        else: die("Bug")
    else: die(f"Unknown action: {action}")

if __name__ == "__main__":
    main()
